# -*- coding: utf-8 -*-
"""LOL-Model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_sfMJ_7TQ6Z1LLUGXlFxBfMU3qCRJflf

<center> <h1>Predicting wins in League of Legends</h1> </center>
<center> <h2>Machine Learning&Deep Learning Class</h2> </center>

https://www.kaggle.com/paololol/league-of-legends-ranked-matches

## Data Fields

### Below you can find the full list of variables present in the data set

#### Champs data set
- name - In-game name of the champion
- id - ID of the champion
#### Matches data set
- id - Match ID
- gameid - Game ID
- platformid - Platform ID
- queueid - Queue ID
- seasonid - Season ID (number of the following seasons: they are enumerated in LOL)
- duration - Duration of the match
- creation - ?
- version - Version of the game
#### Participants data set
- id - ID of the player
- matchid - Match ID
- player - ID of the player in the team (from 1 to 10; where 1-5 is one team and 5-10 is another)
- championid - ID of the champion which player took
- ss1 - Summoner spell on D (should be Flash)
- ss2 - Summoner spell on F (should not be Flash)
- role - SOLO for top and mid, NONE for jungle, DUO_CARRY or DUO_SUPPORT for botlane
- position - bot/jungle/top/mid
#### Stats1&2 data set
- id - ID of the player
- win - Binary variable 0 - lose, 1 - win
- item1 - ID of the first item bought
- item2 - ID of the second item bought
- item3 - ID of the third item bought
- item4 - ID of the fourth item bought
- item5 - ID of the fifth item bought
- item6 - ID of the sixth item bought
- trinket - ?
- kills - How many kills player aquired
- deaths - How many times player died
- assists - How many assists player aquired
- largestkillingspree - Largest killing-spree player aquired
- largestmultikill - Largest multi-kill player aquired
- killingsprees - How many killing-sprees player had
- longesttimespentliving - Longest time the player stayed alive
- doublekills - How many double kills player aquired
- triplekills - How many triple kills player aquired
- quadrakills - How many quadra kills player aquired
- pentakills - How many penta kills player aquired
- legendarykills - How many legendary kills player aquired
- totdmgdealt - How much total damage player dealt
- magicdmgdealt - How much magic damage player dealt
- physicaldmgdealt - How much physical damage player dealt
- truedmgdealt - How much true damage player dealt (surpassing armor)
- largestcrit - Largest critical damage player dealt
- totaldmgtochamp - Total damage dealt to other champions
- magicdmgtochamp - Magic damage dealt to other champions
- physdmgtochamp - Physical damage dealt to other champions
- truedmgtochamp - True damage dealt to other champions (surpassing armor)
- totheal - Total heal received
- totunitshealed - How many allies player healed
- dmgselfmit - ?
- dmgtoobj - Damage dealt to NPCs
- dmgtoturrets - Damage dealt to turrets
- visionscore - ?
- timecc - EMPTY
- totdmgtaken - Total damage received
- magicdmgtaken - Magic damage received
- physdmgtaken - Physical damage received
- truedmgtaken - True damage received (surpassing armor)
- goldearned - Amount of gold aquired in total
- goldspent - Amount of gold spent in total
- turretkills - How many turrets player last hit
- inhibkills - ?
- totminionskilled - How many minions player killed
- neutralminionskilled - ?
- ownjunglekills - How many jungle monsters on the player side of the map player killed
- enemyjunglekills - How many jungle monsters on the opposite side of the map player killed
- totcctimedealt - Total champion controll time player hold other players
- champlvl - Max level of the players champ in the match
- pinksbought - ?
- wardsbought - ?
- wardsplaced - ?
- wardskilled - ?
- firstblood - Binary variable: Does player was the first one to kill enemy champ in the match
#### Teamstats data set

Data set consisted mostly of binary variables, which give us info does team manage to do following things first
- matchid - Match ID
- teamid - Team ID (either 100 or 200)
- firstblood - First blood
- firsttower - First turret
- firstinhib - First inhibitor
- firstbaron - First baron killed
- dragonkills - First dragon killed
- firstharry - First herald killed
- towerkills - Tower destroyed total
- inhibkills - Inhibitors destroyed total
- baronkills - Baron killed total
- dragonkills - Dragon killed total
- harrykills - Herald killed total

#### Teambans data set

- matchid - Match ID
- teamid - Team ID
- championid - Champion ID
- banturn - Which champion was banned first/second/third - max 6

![](https://s.redefine.pl/file/o2/redefine/cp/kn/knmqd3dfmtog1sp4a3z8zvn9rer4gjgo.png)

## League of Legends: An Overview

League of Legends is a fast-paced, competitive online game that blends the speed and intensity of an RTS with RPG elements. Two teams of powerful champions, each with a unique design and playstyle, battle head-to-head across multiple battlefields and game modes. With an ever-expanding roster of champions, frequent updates and a thriving tournament scene, League of Legends offers endless replayability for players of every skill level.

## Aim of the project

<b>The aim of this project is to use machine learning to build classification models which will predict the outcome of the match - win or lose, basing on the given data.</b>

The second aim is to compare these three different models and check which of them outperforms others

# Data preparation
## Libraries
"""

# Standard stuff
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from random import shuffle

# For saving the models
import pickle

# XGBoost
import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV, KFold # For CV
from scipy import stats # F1 score

# Random forest
import sklearn as sk
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
import sklearn.metrics as metrics
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Tensorflow and Keras
import tensorflow as tf
import tensorflow.python.keras
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.callbacks import History 
from tensorflow.python.keras.optimizers import SGD

pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
pd.set_option('display.float_format', lambda x: '%.2f' %x)
warnings.filterwarnings("ignore")

# Center all plots by CSS
from IPython.core.display import HTML

HTML("""
<style>
.output_png {
    display: table-cell;
    text-align: center;
    margin:auto;
    }
.prompt 
    display:none;
}  
</style>
""")

"""## Data import"""

from google.colab import drive
drive.mount('/LOL-Model')

# Champions and ID
champions = pd.read_csv('/LOL-Model/My Drive/LOL-Model/data/champs.csv')

# Matches
matches = pd.read_csv('/LOL-Model/My Drive/LOL-Model/data/matches.csv')

# Player info
playersInfo = pd.read_csv('/LOL-Model/My Drive/LOL-Model/data/participants.csv')

# Stats
stat1 = pd.read_csv('/LOL-Model/My Drive/LOL-Model/data/stats1.csv', low_memory=False)
stat2 = pd.read_csv('/LOL-Model/My Drive/LOL-Model/data/stats2.csv', low_memory=False)
stats = stat1.append(stat2)

# Team - bans
bannedChampion = pd.read_csv('/LOL-Model/My Drive/LOL-Model/data/teambans.csv')

# Team - stats 
teamStats = pd.read_csv('/LOL-Model/My Drive/LOL-Model/data/teamstats.csv')

stats.describe()

teamStats.describe()

teamStats.describe()

playersInfo.describe()

matches.describe()

"""Our data set consists of 6 smaller data sets, which we will have to combine somehow. In total our data set consists of 2 million records and 91 variables, which gives us 182 000 000 data points!!! Quite a big data set...

As one could spot, data is filled well, there are not really many of useless columns, and missing data - so far so goood.

## Explanatory Data Analysis (EDA)
In this part we will perform some Data Engineering, Visualisations etc. to make our variables more efficient for future purposes
![](https://www.gre.ac.uk/__data/assets/image/0011/1191953/analysis-banner.jpg)

### Feature Engineering
First we will have to deal with this split in data. 
- We need to merge data about players performance into one collumn allong with champs,stats and matches data.
- Next we will do the same for teams. 
- Finally we will merge both of them together to create one big ultimate data set!

But, first things first - lets create Players Table which combines players stats, champions they used and information about matches they played in

### Players
"""

# Merge player id with their stats
players = pd.merge(playersInfo, stats, on = ['id'], how = 'left', suffixes=('', '_y'))

# Add the champion name
players = pd.merge(players, champions, how = 'left', left_on = 'championid', right_on = 'id', suffixes=('', '_y'))

# Add the match technical info
players = pd.merge(players, matches, how = 'left', left_on = 'matchid', right_on = 'id', suffixes=('', '_y'))

# Add 1-5 players to team 1, 6-10 to team 2
players['team'] = players['player'].apply(lambda x: 1 if x <= 5 else 2)

#renaming champ collumn name
players.rename(columns={'name': 'champ_chosen'}, inplace=True)

players.head()

# Drop columns
columnsToDrop = ['id', 'championid', 'ss1', 'ss2', 'version','wardsbought', 'id_y', 'gameid','seasonid','platformid','creation','version','seasonid','queueid','platformid']
players.drop(columns = columnsToDrop, inplace = True, errors = 'ignore')

# Drop the 3 rows
players = players.dropna()

# Change floats to ints
for col in players.columns:
    if isinstance(players[col][1], float):
        players[col] = players[col].astype('int64')

# Remove cancelled matches
players = players.query("duration > 100")

# Trimming outliers
players.loc[players.kills > 20, 'players'] = 20
players.loc[players.deaths > 15, 'deaths'] = 15
players.loc[players.assists > 30, 'assists'] = 30

# Changing variables to binary
players.loc[players.doublekills > 1,'doublekills'] = 1
players.loc[players.triplekills > 1,'triplekills'] = 1
players.loc[players.quadrakills > 1,'quadrakills'] = 1
players.loc[players.pentakills > 1,'pentakills'] = 1
players.loc[players.legendarykills > 1,'legendarykills'] = 1

players.head(20)

"""### Teams"""

#Merge teambans with champion id to get insight about champ name
bannedChampion = pd.merge(bannedChampion, champions, how = 'left', left_on = 'championid', right_on = 'id', suffixes=('', '_y'))
bannedChampion.drop(columns = ['id','championid'], inplace = True, errors = 'ignore')

#renaming champ banned collumn name
players.rename(columns={'name': 'champ_banned'}, inplace=True)

bannedChampion.head(15)

#Merging teamstats with players data frame
repeatedteamStats = pd.DataFrame(np.repeat(teamStats.values,5,axis=0))
repeatedteamStats.columns = teamStats.columns

players = pd.concat([players.reset_index(drop=True), repeatedteamStats], axis=1)
players.drop(columns = ['players','matchid'], inplace = True, errors = 'ignore')

"""### Final cleaning"""

#Retrieving finall position of the player in game
def final_position(row):
    if row['role'] in ('DUO_SUPPORT', 'DUO_CARRY'):
        return row['role']
    else:
        return row['position']

players['adjposition'] = players.apply(final_position, axis = 1) 

#dropping unnecessary columns
players.drop(columns = ['matchid','role','position','team','teamid'], inplace = True, errors = 'ignore')

#move last collumn to the beginning
cols = list(players)

# move the column to the beginning of list using index, pop and insert
cols.insert(1, cols.pop(cols.index('adjposition')))
players = players.ix[:, cols]

cols.insert(2, cols.pop(cols.index('champ_chosen')))
players = players.ix[:, cols]

#remove duplicates
players = players.loc[:,~players.columns.duplicated()]

players.head(20)

"""Now as we have our data prepared lets check for NA's and take a look at final describe"""

for i in players.columns:
    print(i,":",players[i].isna().sum()/len(players[i]))

"""As we can see we do not have much of them, so let jus drop them all at one"""

players = players.dropna()

teamStats.tail(20)

"""Quite big data set anyway...

## Plotting
![](https://images.unsplash.com/photo-1434626881859-194d67b2b86f?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1053&q=80)

We plotted as much as possible to see the data in visual form, mostly because we are visual learners and it's easier to interpret for us than just standard table

We will also try not to plot everything, only those variables that we think are crucial to plot

### First lets check if our data set is balanced
"""

plt.figure(figsize=(20,10))
plot = sns.countplot(players['win'])
for ind, label in enumerate(plot.get_xticklabels()):
    if ind % 25 == 0:  # every 10th label is kept
        label.set_visible(True)
    else:
        label.set_visible(False)
plt.title("Is our data balanced?",fontsize=30)
plot.set_xlabel("Win/Loss matches Count",fontsize=15)
plot.tick_params(labelsize=10)
plt.show()

"""Yep... it is! 
That is a good sign for modelling, because we do not have to do any kind of undersampling
### Individual Player Data
"""

# Let's take a look at the distribution of the most important variables
fig = plt.figure(figsize=(15,60))

cols = ('adjposition',
        'kills',
        'deaths',
        'assists',
        'duration',
        'largestkillingspree')

for i, col in enumerate(cols):
    ax = fig.add_subplot(20, 2, i + 1)

    if len(players[col].unique()) > 20:
        sns.distplot(players[col].dropna())
    else:
        sns.countplot(players[col].dropna())

fig.tight_layout(pad=3)

"""As we can see we have some outliers here:
   - 0 in duration means that match was cancelled - we have to delete that records
   - we will also trim outliers:
       - largestkillingspree: lower than 15
       - kills: lower than 25
       - duration: lower than 3500 and higher than 0
"""

players = players.query("duration<3500 and duration>0")
players = players.query("largestkillingspree<15")
players = players.query("kills<25")

fig = plt.figure(figsize=(15,80))

cols = ('totdmgdealt',
        'totdmgtochamp',
        'totdmgtaken',
        'turretkills',
        'totminionskilled',
        'neutralminionskilled')

for i, col in enumerate(cols):
    ax = fig.add_subplot(20, 2, i + 1)

    if len(players[col].unique()) > 20:
        sns.distplot(players[col].dropna())
    else:
        sns.countplot(players[col].dropna())

fig.tight_layout(pad=3)

"""As we can see we have again some outliers here:
   - total damage dealt are pretty high numbers, thats why the chart looks like this..
   - we will also trim outliers:
       - totdmgdealt: higher than 0 (we trim afk players) and lower than 400000
       - totdmgtochamp: lower than 60000
       - totdmgtaken: lower than 80000 and higher than 0 (we trim afk players)
       - turretkills: lower than 7
       - totminionskilled: lower than 400
       - neutralminionskilled: lower than 100
"""

players = players.query("totdmgdealt>0 and totdmgdealt<400000")
players = players.query("totdmgtochamp<60000")
players = players.query("totdmgtaken<80000 and totdmgtaken>0")
players = players.query("turretkills<7")
players = players.query("totminionskilled<400")
players = players.query("neutralminionskilled<100")

fig = plt.figure(figsize=(15,80))

cols = ('goldearned',
        'goldspent')

for i, col in enumerate(cols):
    ax = fig.add_subplot(20, 2, i + 1)

    if len(players[col].unique()) > 20:
        sns.distplot(players[col].dropna())
    else:
        sns.countplot(players[col].dropna())

fig.tight_layout(pad=3)

"""Again we have to drop afk players!"""

players = players.query("goldearned>0 and goldearned<25000")
players = players.query("goldspent>0 and goldspent<25000")

fig = plt.figure(figsize=(16,120))

cols = ('champlvl',
       'visionscore')

for i, col in enumerate(cols):
    ax = fig.add_subplot(20, 2, i + 1)

    if len(players[col].unique()) > 20:
        sns.distplot(players[col].dropna())
    else:
        sns.countplot(players[col].dropna())

fig.tight_layout(pad=3)

"""### Team Data"""

fig = plt.figure(figsize=(15,80))

cols = ('firstblood',
        'firsttower',
        'firstinhib',
        'firstbaron',
        'firstdragon',
        'firstharry')

for i, col in enumerate(cols):
    ax = fig.add_subplot(20, 2, i + 1)

    if len(players[col].unique()) > 20:
        sns.distplot(players[col].dropna())
    else:
        sns.countplot(players[col].dropna())

fig.tight_layout(pad=3)

"""- Our data here in terms of firstblood and firsttower should be balanced, because in-game there is always a team which does this first - It would be a great problem if our aim was to predict outcome of the match per team
- However, as our analysis is per player, we dont mind and we can leave it as it is
"""

fig = plt.figure(figsize=(15,100))

cols = ('towerkills',
        'inhibkills',
        'baronkills',
        'dragonkills',
        'harrykills')

for i, col in enumerate(cols):
    ax = fig.add_subplot(20, 2, i + 1)

    if len(players[col].unique()) > 20:
        sns.distplot(players[col].dropna())
    else:
        sns.countplot(players[col].dropna())

"""And our final outliers to clear:
   - harrykills: less than 2
   - dragonkills: less than 6
   - baronkills: less than 3
   - inhibkills: less than 4
   - towerkills: less than 12
"""

players = players.query("harrykills<2")
players = players.query("dragonkills<6")
players = players.query("baronkills<3")
players = players.query("inhibkills<4")
players = players.query("towerkills<12")

"""## Investigating Correlations among variables
Here we will investigate correlation among our variables
"""

correlationValues = players.corr()

f,ax = plt.subplots(figsize=(25, 25))
sns.heatmap(pd.DataFrame(correlationValues),
            linewidths = 0.05,
            fmt= '.1f',
            ax = ax,
           center = 0)
plt.show()

"""## Model Estimation

![](https://cdn.lynda.com/course/645050/645050-636700308369503992-16x9.jpg)
Let's look at the data again
"""

players.head(10)

"""We can create a list to hold our results"""

results = []

"""Let's split the data into x and y"""

x =  players.drop('win', axis = 1)
y = players['win']

"""Next, we can create dummy variables from the position column, and preprocess the data by scaling it to 0 mean and unit variance"""

# For now drop champion chosen, beacuse it would create too much dummies
x = x.drop('champ_chosen', axis = 1)

# Convert categorical to dummies
x = pd.get_dummies(x)

#Preprocessess data
scaler = StandardScaler()
x = scaler.fit_transform(x)

"""Split the data into training (70%) and testing (30%) sets."""

#Test-train split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .3)

"""# Random forest

We use 200 estimators and limit the depth of the tree to avoid overfitting. The number of jobs is equal to maximum and the optimization criterion is entropy (maximize information gain)
"""

if False:
    rf = RandomForestClassifier(n_estimators = 200,
                            n_jobs = -1,
                            max_depth = 7,
                            criterion = "entropy",
                            random_state = 1,
                            verbose = True,
                            )
    rf.fit(x_train, y_train)
    pickle.dump(rf, open('/LOL-Model/My Drive/LOL-Model//models/rf.sav', 'wb'))

"""We can load the trainde model from a file"""

rf = pickle.load(open('/LOL-Model/My Drive/LOL-Model//models/rf.sav', 'rb'))

"""Make a prediction on the test set"""

preds = rf.predict_proba(x_test)

preds = [a[1] for a in preds[1]]
preds = np.asarray(preds)
preds2 = [1 if a > 0.5 else 0 for a in preds]

"""Then, we can see the relative importance of certain features"""

x2 = players.drop('win', axis = 1)
x2 = x2.drop('champ_chosen', axis = 1)
x2 = pd.get_dummies(x2)

important_features = pd.DataFrame( list( zip(x2.columns, rf.feature_importances_) ) )
important_features.sort_values(by = 1, axis = 0, ascending = False, inplace = True)
important_features.reset_index(inplace=True)
important_features.drop('index', 1, inplace = True)
important_features.columns = ['Feature', 'Relative importance']
important_features.head(10)

"""And the ROC curve."""

# calculate scores
auc = metrics.roc_auc_score(y_test, preds)
# summarize scores
print('AUC=%.3f' % (auc))
# calculate roc curves
fpr, tpr, _ = metrics.roc_curve(y_test, preds)
# plot the roc curve for the model
plt.plot(fpr, tpr, marker='.')
plt.show()

round(sk.metrics.accuracy_score(y_test.values, preds2), 3)

"""In the end, add the results to the list"""

accu = round(sk.metrics.accuracy_score(y_test.values, preds2), 3)
rmse = np.sqrt(mean_squared_error(y_test, preds))
auc = metrics.roc_auc_score(y_test, preds)
results.append(['Random forest', accu, rmse, auc])

"""## XGBoost

Let's run the XGBoost algorithm, using mainly the default parameters
"""

if False: 
  xg_reg = xgb.XGBRegressor(objective ='binary:logistic',  
                          n_estimators = 10)
  xg_reg.fit(x_train,
            y_train,
            verbose=True)
  pickle.dump(xg_reg, open('/LOL-Model/My Drive/LOL-Model//models/xg_reg.sav', 'wb'))

xg_reg = pickle.load(open('/LOL-Model/My Drive/LOL-Model//models/xg_reg.sav', 'rb'))

"""Then, we calculate the predictions"""

preds = xg_reg.predict(x_test)

"""As we can see, they have a typical spread - most on 0 and 1, and some scattered in between"""

ax = sns.distplot(preds)

"""For the XGBoost the ROC curve is much more interesting"""

# calculate scores
auc = metrics.roc_auc_score(y_test, preds)
# summarize scores
print('AUC=%.3f' % (auc))
# calculate roc curves
fpr, tpr, _ = metrics.roc_curve(y_test, preds)
# plot the roc curve for the model
plt.plot(fpr, tpr, marker='.')
plt.show()

"""We make a simple cut at 0.5"""

preds2 = [1 if a > 0.5 else 0 for a in preds]

"""The results are quite balanced"""

ax = sns.distplot(preds2)

"""The accuracy is also good, but it's lower than the Random Forest. Probably due to the lack of hyperparameter tuning"""

round(sk.metrics.accuracy_score(y_test.values, preds2), 3)

rmse = np.sqrt(mean_squared_error(y_test, preds))
print("RMSE: %f" % (rmse))

"""The errors in the confusion matrix is quite balanced"""

pd.crosstab(y_test.values, np.asarray(preds2))

"""Finally, lets add to the results"""

accu = round(sk.metrics.accuracy_score(y_test.values, preds2), 3)
rmse = np.sqrt(mean_squared_error(y_test, preds))
auc = metrics.roc_auc_score(y_test, preds)
results.append(['XGBoost', accu, rmse, auc])

"""## Cross validation of the XGBoost"""

# We have to take a sample, otherwise it takes forever 
y_temp = y_train.reset_index(drop = True)
idx = np.random.choice(np.arange(len(x_train)), 10000, replace=False)
x_sample = x_train[idx]
y_sample = y_temp[idx]

if False:
  xgb_cv = xgb.XGBClassifier(objective = 'binary:logistic')
  # Sample parameters from www:
  param_dist = {'n_estimators': stats.randint(150, 500),
                'learning_rate': stats.uniform(0.01, 0.07),
                'subsample': stats.uniform(0.3, 0.7),
                'max_depth': [3, 4, 5, 6, 7, 8, 9],
                'colsample_bytree': stats.uniform(0.5, 0.45),
                'min_child_weight': [1, 2, 3]
              }
              
  clf = RandomizedSearchCV(xgb_cv, # The model
                          param_distributions = param_dist, # Parameter space
                          n_iter = 35, # How many samples to take
                          scoring = 'accuracy', # We use accuaracy everywhere
                          error_score = 0, # Default for wrong model estiamations
                          verbose = 10, 
                          n_jobs = -1, # Use all cores
                          return_train_score = True # Return scores with results, slightly slower
                           )
  clf.fit(x_sample, y_sample)
  pickle.dump(clf, open('/LOL-Model/My Drive/LOL-Model//models/xg_cv.sav', 'wb'))

clf = pickle.load(open('/LOL-Model/My Drive/LOL-Model//models/xg_cv.sav', 'rb'))

cv_results = pd.DataFrame(clf.cv_results_)
scores = pd.DataFrame({"test" : cv_results.mean_test_score, "train" : cv_results.mean_train_score})
ax = sns.scatterplot(x="test", y="train", data = scores)

"""Some models seem to be overfitting on the train set, but all results are better than the non-optimized XGBoost."""

xg_best = clf.best_estimator_

"""The best model takes a 88% sample of the columns, has a learning rate of 0.07, max depth of 8  and the number of estimators is 440. The increase in accuracy in comprarison to the previous model is most probably due to the number of estimators (440 vs just 10)."""

preds = xg_best.predict_proba(x_test)

preds = [a[1] for a in preds]
preds = np.asarray(preds)
preds2 = [1 if a > 0.5 else 0 for a in preds]

# calculate scores
auc = metrics.roc_auc_score(y_test, preds)
# summarize scores
print('AUC=%.3f' % (auc))
# calculate roc curves
fpr, tpr, _ = metrics.roc_curve(y_test, preds)
# plot the roc curve for the model
plt.plot(fpr, tpr, marker='.')
plt.show()

ax = sns.distplot(preds)

round(sk.metrics.accuracy_score(y_test.values, preds2), 3)

accu = round(sk.metrics.accuracy_score(y_test.values, preds2), 3)
rmse = np.sqrt(mean_squared_error(y_test, preds))
auc = metrics.roc_auc_score(y_test, preds)
results.append(['XGBoost Optimized', accu, rmse, auc])

"""# Neural network

Now let us finish with a neural network. It consists of 4 dense layers of 100 neurons each, with relu activation functions. In the end there are 2 output nodes with softmax activations (since we want results from 0 to 1). The model will use 10% of data as an internal test set
"""

if False:
  #Build Neural Network 
  n_cols = x_train.shape[1]
  y_train2 = tf.keras.utils.to_categorical(y_train, num_classes=2)
  hist = History()

  model = Sequential()

  model.add(Dense(100, activation='relu', input_dim = n_cols))
  model.add(Dense(100, activation='relu'))
  model.add(Dense(100, activation='relu'))
  model.add(Dense(100, activation='relu'))


  model.add(Dense(2, activation='softmax'))

  model.compile(optimizer='adam', 
                loss='binary_crossentropy', 
                metrics=['accuracy'])

  model.fit(x_train, y_train2, epochs = 20, validation_split = .1, callbacks = [hist])
  
  # Save the model
  model.save('/LOL-Model/My Drive/LOL-Model//models/nn.h5')

  with open('/LOL-Model/My Drive/LOL-Model//models/nn_history', 'wb') as file_pi:
      pickle.dump(hist.history, file_pi)

"""Lets load the trained model and the history"""

model = tf.keras.models.load_model('/LOL-Model/My Drive/LOL-Model//models/nn.h5')

hist = pickle.load(open('/LOL-Model/My Drive/LOL-Model/models/nn_history', "rb"))

"""Lets examin accuracy first. As we can see, the train set improves with each epoch, but for the test set the results level off. This indicates that the model is ovefitting after about 3-4 epochs."""

y_pred = model.predict(x_test)
y_pred = y_pred[:,1]

"""The ROC curve is almost unreal, including the AUC score"""

# calculate scores
auc = metrics.roc_auc_score(y_test, y_pred)
# summarize scores
print('AUC=%.3f' % (auc))
# calculate roc curves
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)
# plot the roc curve for the model
plt.plot(fpr, tpr, marker='.')
plt.show()

plt.plot(hist['acc'], color = 'red')
plt.plot(hist['val_acc'], color = 'blue')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

"""This is confirmed when we examin the loss values. Test set falls up to 4 epoch, and then is quite constant."""

plt.plot(hist['loss'], color = 'red')
plt.plot(hist['val_loss'], color = 'blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

"""Let us add the results. We can use the overfitted ones because in reality the difference is very small (about 0.01)"""

auc = metrics.roc_auc_score(y_test, y_pred)
y_pred = np.round(y_pred)
accu = round(sk.metrics.accuracy_score(y_test.values, y_pred), 3)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
results.append(['Neural network', accu, rmse, auc])

"""# Bonus: testing with some suspicious variables removed"""

x =  players.drop(['win',
                   'towerkills',
                   'firstinhib',
                   'turretkills',
                   'dmgtoturrets',
                   'inhibkills',
                   'champ_chosen'], axis = 1)
y = players['win']

# Convert categorical to dummies
x = pd.get_dummies(x)

#Preprocessess data
scaler = StandardScaler()
x = scaler.fit_transform(x)
#Test-train split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .3)

if False:
  xg_reg = xgb.XGBRegressor(objective ='binary:logistic',  
                            n_estimators = 20)

  xg_reg.fit(x_train,
            y_train,
            verbose=True)
  pickle.dump(xg_reg, open('/LOL-Model/My Drive/LOL-Model//models/xg_reg_removed.sav', 'wb'))

xg_reg = pickle.load(open('/LOL-Model/My Drive/LOL-Model//models/xg_reg_removed.sav', 'rb'))

preds = xg_reg.predict(x_test)
# calculate scores
auc = metrics.roc_auc_score(y_test, preds)
# summarize scores
print('AUC=%.3f' % (auc))
# calculate roc curves
fpr, tpr, _ = metrics.roc_curve(y_test, preds)
# plot the roc curve for the model
plt.plot(fpr, tpr, marker='.')
plt.show()

preds2 = [1 if a > 0.5 else 0 for a in preds]

accu = round(sk.metrics.accuracy_score(y_test.values, preds2), 3)
rmse = np.sqrt(mean_squared_error(y_test, preds))
auc = metrics.roc_auc_score(y_test, preds)
results.append(['XGBoost removed suspicious', accu, rmse, auc])

"""## Bonus: don't drop heroes"""

x =  players.drop('win', axis = 1)
y = players['win']

# Convert categorical to dummies
x = pd.get_dummies(x)

#Preprocessess data
scaler = StandardScaler()
x = scaler.fit_transform(x)
#Test-train split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .3)

n_cols = x_train.shape[1]
y_train2 = tf.keras.utils.to_categorical(y_train, num_classes=2)

hist = History()

model = Sequential()

model.add(Dense(100, activation='relu', input_dim = n_cols))
model.add(Dense(100, activation='relu'))
model.add(Dense(100, activation='relu'))
model.add(Dense(100, activation='relu'))

model.add(Dense(2, activation='softmax'))

model.compile(optimizer='adam', 
              loss='binary_crossentropy', 
              metrics=['accuracy'])

model.fit(x_train, y_train2, epochs = 20, validation_split = .1, callbacks = [hist])
model.save('/LOL-Model/My Drive/LOL-Model//models/nn3.h5')

with open('/LOL-Model/My Drive/LOL-Model//models/nn3_history', 'wb') as file_pi:
  pickle.dump(hist.history, file_pi)

model = tf.keras.models.load_model('/LOL-Model/My Drive/LOL-Model//models/nn3.h5')

hist = pickle.load(open('/LOL-Model/My Drive/LOL-Model/models/nn3_history', "rb"))

y_pred = model.predict(x_test)
y_pred = y_pred[:,1]

# calculate scores
auc = metrics.roc_auc_score(y_test, y_pred)
# summarize scores
print('AUC=%.3f' % (auc))
# calculate roc curves
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)
# plot the roc curve for the model
plt.plot(fpr, tpr, marker='.')
plt.show()

plt.plot(hist['accuracy'], color = 'red')
plt.plot(hist['val_accuracy'], color = 'blue')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(hist['loss'], color = 'red')
plt.plot(hist['val_loss'], color = 'blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

auc = metrics.roc_auc_score(y_test, y_pred)
y_pred = np.round(y_pred)
accu = round(sk.metrics.accuracy_score(y_test.values, y_pred), 3)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
results.append(['Neural network with heroes', accu, rmse, auc])

"""# Results!

In our testing, the Neural Network achieved the best results in all categories: Accuracy, MSE and the AUC. The Random Forest had better results than the unoptimized XGBoost, but the results improved significantly with optimization.
"""

results2 = pd.DataFrame(results, copy = True)
results2.columns = ['Method', 'Accuracy', 'Mean squared error', 'Area under curve']
results2.sort_values("Accuracy")

"""When we removed some correlated ('suspicious') variables and retested the XGBoost, the results were weaker (82% vs 84%), but the were still quite good. This leads us to belive there is no information leakage in this variables, and they just help the model the same way other variables help it.

Optimizing the XGBoost with cross validation and hyperparameter tuning (on a 10k sample) increased the results significantly (82% vs 91%). However, we beleive this is mainly due to the significantly higher number of estimators (440 vs 10). We were able to run this models thanks to the sampling - running the whole dataset on the 440 estimators would take probably more than a day.

We tried to run the neural network with the heroes column converted to binary columns, but the results were the same. However, the model too considerably longer to train.
"""