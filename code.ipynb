{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1>Predicting wins in League of Legends</h1> </center>\n",
    "<center> <h2>Machine Learning&Deep Learning Class</h2> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/paololol/league-of-legends-ranked-matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below you can find the full list of variables present in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Champs data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- name - In-game name of the champion\n",
    "- id - ID of the champion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matches data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- id - Match ID\n",
    "- gameid - Game ID\n",
    "- platformid - Platform ID\n",
    "- queueid - Queue ID\n",
    "- seasonid - Season ID (number of the following seasons: they are enumerated in LOL)\n",
    "- duration - Duration of the match\n",
    "- creation - ?\n",
    "- version - Version of the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Participants data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- id - ID of the player\n",
    "- matchid - Match ID\n",
    "- player - ID of the player in the team (from 1 to 10; where 1-5 is one team and 5-10 is another)\n",
    "- championid - ID of the champion which player took\n",
    "- ss1 - Summoner spell on D (should be Flash)\n",
    "- ss2 - Summoner spell on F (should not be Flash)\n",
    "- role - SOLO for top and mid, NONE for jungle, DUO_CARRY or DUO_SUPPORT for botlane\n",
    "- position - bot/jungle/top/mid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats1&2 data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- id - ID of the player\n",
    "- win - Binary variable 0 - lose, 1 - win\n",
    "- item1 - ID of the first item bought\n",
    "- item2 - ID of the second item bought\n",
    "- item3 - ID of the third item bought\n",
    "- item4 - ID of the fourth item bought\n",
    "- item5 - ID of the fifth item bought\n",
    "- item6 - ID of the sixth item bought\n",
    "- trinket - ?\n",
    "- kills - How many kills player aquired\n",
    "- deaths - How many times player died\n",
    "- assists - How many assists player aquired\n",
    "- largestkillingspree - Largest killing-spree player aquired\n",
    "- largestmultikill - Largest multi-kill player aquired\n",
    "- killingsprees - How many killing-sprees player had\n",
    "- longesttimespentliving - Longest time the player stayed alive\n",
    "- doublekills - How many double kills player aquired\n",
    "- triplekills - How many triple kills player aquired\n",
    "- quadrakills - How many quadra kills player aquired\n",
    "- pentakills - How many penta kills player aquired\n",
    "- legendarykills - How many legendary kills player aquired\n",
    "- totdmgdealt - How much total damage player dealt\n",
    "- magicdmgdealt - How much magic damage player dealt\n",
    "- physicaldmgdealt - How much physical damage player dealt\n",
    "- truedmgdealt - How much true damage player dealt (surpassing armor)\n",
    "- largestcrit - Largest critical damage player dealt\n",
    "- totaldmgtochamp - Total damage dealt to other champions\n",
    "- magicdmgtochamp - Magic damage dealt to other champions\n",
    "- physdmgtochamp - Physical damage dealt to other champions\n",
    "- truedmgtochamp - True damage dealt to other champions (surpassing armor)\n",
    "- totheal - Total heal received\n",
    "- totunitshealed - How many allies player healed\n",
    "- dmgselfmit - ?\n",
    "- dmgtoobj - Damage dealt to NPCs\n",
    "- dmgtoturrets - Damage dealt to turrets\n",
    "- visionscore - ?\n",
    "- timecc - EMPTY\n",
    "- totdmgtaken - Total damage received\n",
    "- magicdmgtaken - Magic damage received\n",
    "- physdmgtaken - Physical damage received\n",
    "- truedmgtaken - True damage received (surpassing armor)\n",
    "- goldearned - Amount of gold aquired in total\n",
    "- goldspent - Amount of gold spent in total\n",
    "- turretkills - How many turrets player last hit\n",
    "- inhibkills - ?\n",
    "- totminionskilled - How many minions player killed\n",
    "- neutralminionskilled - ?\n",
    "- ownjunglekills - How many jungle monsters on the player side of the map player killed\n",
    "- enemyjunglekills - How many jungle monsters on the opposite side of the map player killed\n",
    "- totcctimedealt - Total champion controll time player hold other players\n",
    "- champlvl - Max level of the players champ in the match\n",
    "- pinksbought - ?\n",
    "- wardsbought - ?\n",
    "- wardsplaced - ?\n",
    "- wardskilled - ?\n",
    "- firstblood - Binary variable: Does player was the first one to kill enemy champ in the match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teamstats data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set consisted mostly of binary variables, which give us info does team manage to do following things first\n",
    "- matchid - Match ID\n",
    "- teamid - Team ID (either 100 or 200)\n",
    "- firstblood - First blood\n",
    "- firsttower - First turret\n",
    "- firstinhib - First inhibitor\n",
    "- firstbaron - First baron killed\n",
    "- dragonkills - First dragon killed\n",
    "- firstharry - First herald killed\n",
    "- towerkills - Tower destroyed total\n",
    "- inhibkills - Inhibitors destroyed total\n",
    "- baronkills - Baron killed total\n",
    "- dragonkills - Dragon killed total\n",
    "- harrykills - Herald killed total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teambans data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- matchid - Match ID\n",
    "- teamid - Team ID\n",
    "- championid - Champion ID\n",
    "- banturn - Which champion was banned first/second/third - max 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s.redefine.pl/file/o2/redefine/cp/kn/knmqd3dfmtog1sp4a3z8zvn9rer4gjgo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## League of Legends: An Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "League of Legends is a fast-paced, competitive online game that blends the speed and intensity of an RTS with RPG elements. Two teams of powerful champions, each with a unique design and playstyle, battle head-to-head across multiple battlefields and game modes. With an ever-expanding roster of champions, frequent updates and a thriving tournament scene, League of Legends offers endless replayability for players of every skill level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to use Neural Networks, SVM, and DeepLearning models to build classification model which will predict the outcome of the match - win or lose, basing on the given data.\n",
    "\n",
    "The second aim is to compare these three different models and check which of them outperforms others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import gc\n",
    "import re\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from geopy.distance import great_circle\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "from __future__ imports must occur at the beginning of the file (<ipython-input-3-8158c4e96005>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-8158c4e96005>\"\u001b[1;36m, line \u001b[1;32m24\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m from __future__ imports must occur at the beginning of the file\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from random import shuffle\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History \n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' %x)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center all plots by CSS\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    margin:auto;\n",
    "    }\n",
    ".prompt \n",
    "    display:none;\n",
    "}  \n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Champions and ID\n",
    "champions = pd.read_csv('data/champs.csv')\n",
    "\n",
    "# Matches\n",
    "matches = pd.read_csv('data/matches.csv')\n",
    "\n",
    "# Player info\n",
    "playersInfo = pd.read_csv('data/participants.csv')\n",
    "\n",
    "# Stats\n",
    "stat1 = pd.read_csv('data/stats1.csv', low_memory=False)\n",
    "stat2 = pd.read_csv('data/stats2.csv', low_memory=False)\n",
    "stats = stat1.append(stat2)\n",
    "\n",
    "# Team - bans\n",
    "bannedChampion = pd.read_csv('data/teambans.csv')\n",
    "\n",
    "# Team - stats \n",
    "teamStats = pd.read_csv('data/teamstats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teamStats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bannedChampion.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playersInfo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set consists of 6 smaller data sets, which we will have to combine somehow. In total our data set consists of 2 million records and 91 variables, which gives us 182 000 000 data points!!! Quite big data set...\n",
    "\n",
    "As one could spot, data is filled well, there are not really many of useless columns, and missing data - so far so goood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanatory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will perform some Data Engineering, Visualisations etc. to make our variables more efficient for future purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.gre.ac.uk/__data/assets/image/0011/1191953/analysis-banner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will have to deal with this split in data. \n",
    "- We need to merge data about players performance into one collumn allong with champs,stats and matches data.\n",
    "- Next we will do the same for teams. \n",
    "- Finally we will merge both of them together to create one big ultimate data set!\n",
    "\n",
    "But, first things first - lets create Players Table which combines players stats, champions they used and information about matches they played in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge player id with their stats\n",
    "players = pd.merge(playersInfo, stats, on = ['id'], how = 'left', suffixes=('', '_y'))\n",
    "\n",
    "# Add the champion name\n",
    "players = pd.merge(players, champions, how = 'left', left_on = 'championid', right_on = 'id', suffixes=('', '_y'))\n",
    "\n",
    "# Add the match technical info\n",
    "players = pd.merge(players, matches, how = 'left', left_on = 'matchid', right_on = 'id', suffixes=('', '_y'))\n",
    "\n",
    "# Add 1-5 players to team 1, 6-10 to team 2\n",
    "players['team'] = players['player'].apply(lambda x: 1 if x <= 5 else 2)\n",
    "\n",
    "#renaming champ collumn name\n",
    "players.rename(columns={'name': 'champ_chosen'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "columnsToDrop = ['id', 'championid', 'ss1', 'ss2', 'version','wardsbought', 'id_y', 'gameid','seasonid','platformid','creation','version','seasonid','queueid','platformid']\n",
    "players.drop(columns = columnsToDrop, inplace = True, errors = 'ignore')\n",
    "\n",
    "# Drop the 3 rows\n",
    "players = players.dropna()\n",
    "\n",
    "# Change floats to ints\n",
    "for col in players.columns:\n",
    "    if isinstance(players[col][1], float):\n",
    "        players[col] = players[col].astype('int64')\n",
    "\n",
    "# Remove cancelled matches\n",
    "players = players.query(\"duration > 100\")\n",
    "\n",
    "# Trimming outliers\n",
    "players.loc[players.kills > 20, 'players'] = 20\n",
    "players.loc[players.deaths > 15, 'deaths'] = 15\n",
    "players.loc[players.assists > 30, 'assists'] = 30\n",
    "\n",
    "# Changing variables to binary\n",
    "players.loc[players.doublekills > 1,'doublekills'] = 1\n",
    "players.loc[players.triplekills > 1,'triplekills'] = 1\n",
    "players.loc[players.quadrakills > 1,'quadrakills'] = 1\n",
    "players.loc[players.pentakills > 1,'pentakills'] = 1\n",
    "players.loc[players.legendarykills > 1,'legendarykills'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge teambans with champion id to get insight about champ name\n",
    "bannedChampion = pd.merge(bannedChampion, champions, how = 'left', left_on = 'championid', right_on = 'id', suffixes=('', '_y'))\n",
    "bannedChampion.drop(columns = ['id','championid'], inplace = True, errors = 'ignore')\n",
    "\n",
    "#renaming champ banned collumn name\n",
    "players.rename(columns={'name': 'champ_banned'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bannedChampion.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging teamstats with players data frame\n",
    "repeatedteamStats = pd.DataFrame(np.repeat(teamStats.values,5,axis=0))\n",
    "repeatedteamStats.columns = teamStats.columns\n",
    "\n",
    "players = pd.concat([players.reset_index(drop=True), repeatedteamStats], axis=1)\n",
    "players.drop(columns = ['players','matchid'], inplace = True, errors = 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finall cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving finall position of the player in game\n",
    "def final_position(row):\n",
    "    if row['role'] in ('DUO_SUPPORT', 'DUO_CARRY'):\n",
    "        return row['role']\n",
    "    else:\n",
    "        return row['position']\n",
    "\n",
    "players['adjposition'] = players.apply(final_position, axis = 1) \n",
    "\n",
    "#dropping unnecessary columns\n",
    "players.drop(columns = ['matchid','role','position','team','teamid'], inplace = True, errors = 'ignore')\n",
    "\n",
    "#move last collumn to the beginning\n",
    "cols = list(players)\n",
    "\n",
    "# move the column to the beginning of list using index, pop and insert\n",
    "cols.insert(1, cols.pop(cols.index('adjposition')))\n",
    "players = players.ix[:, cols]\n",
    "\n",
    "cols.insert(2, cols.pop(cols.index('champ_chosen')))\n",
    "players = players.ix[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "players = players.loc[:,~players.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as we have our data prepared lets check for NA's and take a look at final describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in players.columns:\n",
    "    print(i,\":\",players[i].isna().sum()/len(players[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we do not have much of them, so let jus drop them all at one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = players.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teamStats.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite big data set anyway..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.unsplash.com/photo-1434626881859-194d67b2b86f?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1053&q=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plotted as much as possible to see the data in visual form, mostly because we are visual learners and it's easier to interpret for us than just standard table\n",
    "\n",
    "We will also try not to plot everything, only those variables that we think are crucial to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First lets check if our data set is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plot = sns.countplot(players['win'])\n",
    "for ind, label in enumerate(plot.get_xticklabels()):\n",
    "    if ind % 25 == 0:  # every 10th label is kept\n",
    "        label.set_visible(True)\n",
    "    else:\n",
    "        label.set_visible(False)\n",
    "plt.title(\"Is our data balanced?\",fontsize=30)\n",
    "plot.set_xlabel(\"Win/Loss matches Count\",fontsize=15)\n",
    "plot.tick_params(labelsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep... it is! \n",
    "- That is a good sign for modelling, because we do not have to do any kind of undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Player Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the distribution of the most important variables\n",
    "fig = plt.figure(figsize=(15,60))\n",
    "\n",
    "cols = ('adjposition',\n",
    "        'kills',\n",
    "        'deaths',\n",
    "        'assists',\n",
    "        'duration',\n",
    "        'largestkillingspree')\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    ax = fig.add_subplot(20, 2, i + 1)\n",
    "\n",
    "    if len(players[col].unique()) > 20:\n",
    "        sns.distplot(players[col].dropna())\n",
    "    else:\n",
    "        sns.countplot(players[col].dropna())\n",
    "\n",
    "fig.tight_layout(pad=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have some outliers here:\n",
    "   - 0 in duration means that match was cancelled - we have to delete that records\n",
    "   - we will also trim outliers:\n",
    "       - largestkillingspree: lower than 15\n",
    "       - kills: lower than 25\n",
    "       - duration: lower than 3500 and higher than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = players.query(\"duration<3500 and duration>0\")\n",
    "players = players.query(\"largestkillingspree<15\")\n",
    "players = players.query(\"kills<25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,100))\n",
    "\n",
    "cols = ('totdmgdealt',\n",
    "        'totdmgtochamp',\n",
    "        'totdmgtaken',\n",
    "        'turretkills',\n",
    "        'totminionskilled',\n",
    "        'neutralminionskilled')\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    ax = fig.add_subplot(20, 2, i + 1)\n",
    "\n",
    "    if len(players[col].unique()) > 20:\n",
    "        sns.distplot(players[col].dropna())\n",
    "    else:\n",
    "        sns.countplot(players[col].dropna())\n",
    "\n",
    "fig.tight_layout(pad=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have again some outliers here:\n",
    "   - total damage dealt are pretty high numbers, thats why the chart looks like this..\n",
    "   - we will also trim outliers:\n",
    "       - totdmgdealt: higher than 0 (we trim afk players) and lower than 400000\n",
    "       - totdmgtochamp: lower than 60000\n",
    "       - totdmgtaken: lower than 80000 and higher than 0 (we trim afk players)\n",
    "       - turretkills: lower than 7\n",
    "       - totminionskilled: lower than 400\n",
    "       - neutralminionskilled: lower than 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = players.query(\"totdmgdealt>0 and totdmgdealt<400000\")\n",
    "players = players.query(\"totdmgtochamp<60000\")\n",
    "players = players.query(\"totdmgtaken<80000 and totdmgtaken>0\")\n",
    "players = players.query(\"turretkills<7\")\n",
    "players = players.query(\"totminionskilled<400\")\n",
    "players = players.query(\"neutralminionskilled<100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,80))\n",
    "\n",
    "cols = ('goldearned',\n",
    "        'goldspent')\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    ax = fig.add_subplot(20, 2, i + 1)\n",
    "\n",
    "    if len(players[col].unique()) > 20:\n",
    "        sns.distplot(players[col].dropna())\n",
    "    else:\n",
    "        sns.countplot(players[col].dropna())\n",
    "\n",
    "fig.tight_layout(pad=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we have to drop afk players!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = players.query(\"goldearned>0 and goldearned<25000\")\n",
    "players = players.query(\"goldspent>0 and goldspent<25000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,120))\n",
    "\n",
    "cols = ('champlvl',\n",
    "       'visionscore')\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    ax = fig.add_subplot(20, 2, i + 1)\n",
    "\n",
    "    if len(players[col].unique()) > 20:\n",
    "        sns.distplot(players[col].dropna())\n",
    "    else:\n",
    "        sns.countplot(players[col].dropna())\n",
    "\n",
    "fig.tight_layout(pad=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,80))\n",
    "\n",
    "cols = ('firstblood',\n",
    "        'firsttower',\n",
    "        'firstinhib',\n",
    "        'firstbaron',\n",
    "        'firstdragon',\n",
    "        'firstharry')\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    ax = fig.add_subplot(20, 2, i + 1)\n",
    "\n",
    "    if len(players[col].unique()) > 20:\n",
    "        sns.distplot(players[col].dropna())\n",
    "    else:\n",
    "        sns.countplot(players[col].dropna())\n",
    "\n",
    "fig.tight_layout(pad=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our data here in terms of firstblood and firsttower should be balanced, because in-game there is always a team which does this first - It would be a great problem if our aim was to predict outcome of the match per team\n",
    "- However, as our analysis is per player, we dont mind and we can leave it as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,100))\n",
    "\n",
    "cols = ('towerkills',\n",
    "        'inhibkills',\n",
    "        'baronkills',\n",
    "        'dragonkills',\n",
    "        'harrykills')\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    ax = fig.add_subplot(20, 2, i + 1)\n",
    "\n",
    "    if len(players[col].unique()) > 20:\n",
    "        sns.distplot(players[col].dropna())\n",
    "    else:\n",
    "        sns.countplot(players[col].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our final outliers to clear:\n",
    "   - harrykills: less than 2\n",
    "   - dragonkills: less than 6\n",
    "   - baronkills: less than 3\n",
    "   - inhibkills: less than 4\n",
    "   - towerkills: less than 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = players.query(\"harrykills<2\")\n",
    "players = players.query(\"dragonkills<6\")\n",
    "players = players.query(\"baronkills<3\")\n",
    "players = players.query(\"inhibkills<4\")\n",
    "players = players.query(\"towerkills<12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating Correlations among variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we will investigate correlation among our variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlationValues = players.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(40, 40))\n",
    "sns.heatmap(pd.DataFrame(correlationValues),\n",
    "            linewidths = 0.05,\n",
    "            fmt= '.1f',\n",
    "            ax = ax,\n",
    "           center = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will try to estimate our models: XGBoost, NeuralNetworks.\n",
    "\n",
    "What we've done here:\n",
    "\n",
    "- Feature selection to fit our model best\n",
    "- Hyperparameters tunning\n",
    "- Base our conclusions on Precision and Recall in addition to Accuracy\n",
    "- Time Efficiency testing for each algorithm\n",
    "\n",
    "Lets begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn.lynda.com/course/645050/645050-636700308369503992-16x9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  players.drop('win', axis = 1)\n",
    "y = players['win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now drop champion chosen, beacuse it would create too much dummies\n",
    "x = x.drop('champ_chosen', axis = 1)\n",
    "\n",
    "# Convert categorical to dummies\n",
    "x = pd.get_dummies(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessess data\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test-train split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Neural Network \n",
    "n_cols = x_train.shape[1]\n",
    "y_train = to_categorical(y_train, 2)\n",
    "\n",
    "hist = History()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(10, activation='relu', input_dim = n_cols))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs = 1, validation_split = .1, callbacks = [hist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =  model.predict(x_test)\n",
    "y_pred = np.round(y_pred[:,1])\n",
    "\n",
    "plt.plot(hist.history['accuracy'], color = 'red')\n",
    "plt.plot(hist.history['val_accuracy'], color = 'blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'], color = 'red')\n",
    "plt.plot(hist.history['val_loss'], color = 'blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fac20ee3dede>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m rf = RandomForestClassifier(n_estimators = 300,\n\u001b[0m\u001b[0;32m      2\u001b[0m                             \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                             \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"entropy\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                             random_state = 1)\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 300,\n",
    "                            n_jobs = -1,\n",
    "                            criterion = \"entropy\",\n",
    "                            random_state = 1)\n",
    "\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\"objective\": \"binary:logistic\",\n",
    "          \"eta\": 0.15,\n",
    "          \"max_depth\": 7,\n",
    "          \"min_child_weight\": 10,\n",
    "          \"silent\": 1,\n",
    "          \"subsample\": 0.7,\n",
    "          \"colsample_bytree\": 0.7,\n",
    "          \"seed\": 1}\n",
    "num_trees=250\n",
    "gbm = xgb.train(params, xgb.DMatrix(train[features], train[\"signal\"]), num_trees)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
